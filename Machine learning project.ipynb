{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-statement\" data-toc-modified-id=\"Problem-statement-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem statement</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Error-Metric-:-Recall\" data-toc-modified-id=\"Error-Metric-:-Recall-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Error Metric : Recall</a></span></li><li><span><a href=\"#Importing-the-required-libraries-and-packages\" data-toc-modified-id=\"Importing-the-required-libraries-and-packages-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Importing the required libraries and packages</a></span></li><li><span><a href=\"#Reading-the-data\" data-toc-modified-id=\"Reading-the-data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Reading the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-all-columns-with-correct-spellings\" data-toc-modified-id=\"Renaming-all-columns-with-correct-spellings-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Renaming all columns with correct spellings</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summary-Stats\" data-toc-modified-id=\"Summary-Stats-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Summary Stats</a></span></li><li><span><a href=\"#Checking-Datatypes-&amp;-Typecasting-obvious-ones\" data-toc-modified-id=\"Checking-Datatypes-&amp;-Typecasting-obvious-ones-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Checking Datatypes &amp; Typecasting obvious ones</a></span></li><li><span><a href=\"#Analysis-on-Patient-ID-and-Appointment-ID\" data-toc-modified-id=\"Analysis-on-Patient-ID-and-Appointment-ID-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Analysis on Patient ID and Appointment ID</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-Zero-Variance-features-like-Appointment-ID\" data-toc-modified-id=\"Dropping-Zero-Variance-features-like-Appointment-ID-1.5.3.1\"><span class=\"toc-item-num\">1.5.3.1&nbsp;&nbsp;</span>Dropping Zero Variance features like Appointment ID</a></span></li></ul></li><li><span><a href=\"#Analysing-Numerical-features\" data-toc-modified-id=\"Analysing-Numerical-features-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Analysing Numerical features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Handling-Age-Attribute\" data-toc-modified-id=\"Handling-Age-Attribute-1.5.4.1\"><span class=\"toc-item-num\">1.5.4.1&nbsp;&nbsp;</span>Handling Age Attribute</a></span></li><li><span><a href=\"#Effect-of-Age-on-No-show-attribute\" data-toc-modified-id=\"Effect-of-Age-on-No-show-attribute-1.5.4.2\"><span class=\"toc-item-num\">1.5.4.2&nbsp;&nbsp;</span>Effect of Age on No-show attribute</a></span></li></ul></li><li><span><a href=\"#Analysing-Categorical-Columns\" data-toc-modified-id=\"Analysing-Categorical-Columns-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span>Analysing Categorical Columns</a></span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-Analysis-on-Categorical-Columns\" data-toc-modified-id=\"Univariate-Analysis-on-Categorical-Columns-1.5.5.1\"><span class=\"toc-item-num\">1.5.5.1&nbsp;&nbsp;</span>Univariate Analysis on Categorical Columns</a></span></li><li><span><a href=\"#Bining-of-Handicapped-attrribute\" data-toc-modified-id=\"Bining-of-Handicapped-attrribute-1.5.5.2\"><span class=\"toc-item-num\">1.5.5.2&nbsp;&nbsp;</span>Bining of Handicapped attrribute</a></span></li></ul></li><li><span><a href=\"#Chi-Square-test-of-independence\" data-toc-modified-id=\"Chi-Square-test-of-independence-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;</span>Chi-Square test of independence</a></span></li><li><span><a href=\"#Probabilistic-Analysis-of-Categorical-columns-on-No-show\" data-toc-modified-id=\"Probabilistic-Analysis-of-Categorical-columns-on-No-show-1.5.7\"><span class=\"toc-item-num\">1.5.7&nbsp;&nbsp;</span>Probabilistic Analysis of Categorical columns on No show</a></span></li><li><span><a href=\"#Dealing-High-Cardinal-Categorical-Attributes\" data-toc-modified-id=\"Dealing-High-Cardinal-Categorical-Attributes-1.5.8\"><span class=\"toc-item-num\">1.5.8&nbsp;&nbsp;</span>Dealing High Cardinal Categorical Attributes</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generating-new-features-from-scheduled-and-appointment-date-features\" data-toc-modified-id=\"Generating-new-features-from-scheduled-and-appointment-date-features-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Generating new features from scheduled and appointment date features</a></span></li><li><span><a href=\"#Creating-'number-of-days-to-appointment'\" data-toc-modified-id=\"Creating-'number-of-days-to-appointment'-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Creating 'number of days to appointment'</a></span></li><li><span><a href=\"#Typecasting-newly-created-features\" data-toc-modified-id=\"Typecasting-newly-created-features-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>Typecasting newly created features</a></span></li><li><span><a href=\"#Effect-of-num_days-on-No-show-attribute\" data-toc-modified-id=\"Effect-of-num_days-on-No-show-attribute-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>Effect of num_days on No-show attribute</a></span></li><li><span><a href=\"#Effect-of-Scheduled_hour-on-No-show-attribute\" data-toc-modified-id=\"Effect-of-Scheduled_hour-on-No-show-attribute-1.6.5\"><span class=\"toc-item-num\">1.6.5&nbsp;&nbsp;</span>Effect of Scheduled_hour on No-show attribute</a></span></li><li><span><a href=\"#Performing-Chisquare-test-of-independence-on-newly-added-categorical-columns\" data-toc-modified-id=\"Performing-Chisquare-test-of-independence-on-newly-added-categorical-columns-1.6.6\"><span class=\"toc-item-num\">1.6.6&nbsp;&nbsp;</span>Performing Chisquare test of independence on newly added categorical columns</a></span></li><li><span><a href=\"#Probabilistic-Analysis-of-newly-created-features\" data-toc-modified-id=\"Probabilistic-Analysis-of-newly-created-features-1.6.7\"><span class=\"toc-item-num\">1.6.7&nbsp;&nbsp;</span>Probabilistic Analysis of newly created features</a></span></li></ul></li><li><span><a href=\"#Checking--target-feature-distribution\" data-toc-modified-id=\"Checking--target-feature-distribution-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Checking  target feature distribution</a></span><ul class=\"toc-item\"><li><span><a href=\"#Replacing-Yes/No-in-the-target-column-with-1/0\" data-toc-modified-id=\"Replacing-Yes/No-in-the-target-column-with-1/0-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Replacing Yes/No in the target column with 1/0</a></span></li></ul></li></ul></li><li><span><a href=\"#Split-the-data-into-train-and-test\" data-toc-modified-id=\"Split-the-data-into-train-and-test-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Split the data into train and test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-the-attributes-into-numerical-and-categorical-types\" data-toc-modified-id=\"Split-the-attributes-into-numerical-and-categorical-types-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Split the attributes into numerical and categorical types</a></span></li></ul></li><li><span><a href=\"#Preparation-for-Model-buillding\" data-toc-modified-id=\"Preparation-for-Model-buillding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preparation for Model buillding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Imputing-missing-values-with-median\" data-toc-modified-id=\"Imputing-missing-values-with-median-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Imputing missing values with median</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standardize-the-data\" data-toc-modified-id=\"Standardize-the-data-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Standardize the data</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoding-of-categorical-attributes\" data-toc-modified-id=\"One-Hot-Encoding-of-categorical-attributes-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>One Hot Encoding of categorical attributes</a></span></li><li><span><a href=\"#Merging-of-Numerical-and-Categorical-Dataframes\" data-toc-modified-id=\"Merging-of-Numerical-and-Categorical-Dataframes-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>Merging of Numerical and Categorical Dataframes</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#MODEL-BUILDING\" data-toc-modified-id=\"MODEL-BUILDING-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>MODEL BUILDING</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1.Logistic-Regression\" data-toc-modified-id=\"1.Logistic-Regression-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>1.Logistic Regression</a></span></li><li><span><a href=\"#Calculate-Accuracy,-True-Positive-Rate-and-True-Negative-Rates\" data-toc-modified-id=\"Calculate-Accuracy,-True-Positive-Rate-and-True-Negative-Rates-4.0.0.2\"><span class=\"toc-item-num\">4.0.0.2&nbsp;&nbsp;</span>Calculate Accuracy, True Positive Rate and True Negative Rates</a></span></li><li><span><a href=\"#2.Handling-Class-Imbalance-using-Synthetic-Minority-Oversampling-Technique\" data-toc-modified-id=\"2.Handling-Class-Imbalance-using-Synthetic-Minority-Oversampling-Technique-4.0.0.3\"><span class=\"toc-item-num\">4.0.0.3&nbsp;&nbsp;</span>2.Handling Class Imbalance using Synthetic Minority Oversampling Technique</a></span></li><li><span><a href=\"#Calculate-Accuracy,-True-Positive-Rate-and-True-Negative-Rates\" data-toc-modified-id=\"Calculate-Accuracy,-True-Positive-Rate-and-True-Negative-Rates-4.0.0.4\"><span class=\"toc-item-num\">4.0.0.4&nbsp;&nbsp;</span>Calculate Accuracy, True Positive Rate and True Negative Rates</a></span></li><li><span><a href=\"#3.Decision-Tree-Classifier\" data-toc-modified-id=\"3.Decision-Tree-Classifier-4.0.0.5\"><span class=\"toc-item-num\">4.0.0.5&nbsp;&nbsp;</span>3.Decision Tree Classifier</a></span></li></ul></li><li><span><a href=\"#Add-SMOT-with-DT\" data-toc-modified-id=\"Add-SMOT-with-DT-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Add SMOT with DT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculate-Accuracy,-True-Positive-Rate-and-True-Negative-Rates\" data-toc-modified-id=\"Calculate-Accuracy,-True-Positive-Rate-and-True-Negative-Rates-4.0.1.1\"><span class=\"toc-item-num\">4.0.1.1&nbsp;&nbsp;</span>Calculate Accuracy, True Positive Rate and True Negative Rates</a></span></li><li><span><a href=\"#Plotting-feature-importance-using-Decision-Tree\" data-toc-modified-id=\"Plotting-feature-importance-using-Decision-Tree-4.0.1.2\"><span class=\"toc-item-num\">4.0.1.2&nbsp;&nbsp;</span>Plotting feature importance using Decision Tree</a></span></li><li><span><a href=\"#Visualizing-the-tree\" data-toc-modified-id=\"Visualizing-the-tree-4.0.1.3\"><span class=\"toc-item-num\">4.0.1.3&nbsp;&nbsp;</span>Visualizing the tree</a></span></li><li><span><a href=\"#Building-Decision-Tree-Model-using-Feature-Importance\" data-toc-modified-id=\"Building-Decision-Tree-Model-using-Feature-Importance-4.0.1.4\"><span class=\"toc-item-num\">4.0.1.4&nbsp;&nbsp;</span>Building Decision Tree Model using Feature Importance</a></span></li><li><span><a href=\"#Using-only-top-10-features\" data-toc-modified-id=\"Using-only-top-10-features-4.0.1.5\"><span class=\"toc-item-num\">4.0.1.5&nbsp;&nbsp;</span>Using only top 10 features</a></span></li><li><span><a href=\"#Visualizing-Decision-trees\" data-toc-modified-id=\"Visualizing-Decision-trees-4.0.1.6\"><span class=\"toc-item-num\">4.0.1.6&nbsp;&nbsp;</span>Visualizing Decision trees</a></span></li><li><span><a href=\"#Hyper-parameter-Tuning\" data-toc-modified-id=\"Hyper-parameter-Tuning-4.0.1.7\"><span class=\"toc-item-num\">4.0.1.7&nbsp;&nbsp;</span>Hyper-parameter Tuning</a></span></li></ul></li><li><span><a href=\"#How-many-features-are-required-to-produce-the-same-results-for-the-best-estimators.\" data-toc-modified-id=\"How-many-features-are-required-to-produce-the-same-results-for-the-best-estimators.-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>How many features are required to produce the same results for the best estimators.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-4.0.2.1\"><span class=\"toc-item-num\">4.0.2.1&nbsp;&nbsp;</span>Random Forest Classifier</a></span></li><li><span><a href=\"#Adaboost-Classifier\" data-toc-modified-id=\"Adaboost-Classifier-4.0.2.2\"><span class=\"toc-item-num\">4.0.2.2&nbsp;&nbsp;</span>Adaboost Classifier</a></span></li><li><span><a href=\"#Adaboost-with-important-feautures-only\" data-toc-modified-id=\"Adaboost-with-important-feautures-only-4.0.2.3\"><span class=\"toc-item-num\">4.0.2.3&nbsp;&nbsp;</span>Adaboost with important feautures only</a></span></li><li><span><a href=\"#RESULTS\" data-toc-modified-id=\"RESULTS-4.0.2.4\"><span class=\"toc-item-num\">4.0.2.4&nbsp;&nbsp;</span>RESULTS</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBGiSh-Cd5pY"
   },
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuHI-rfcd5pa"
   },
   "source": [
    "### Prediction of Patient no-show in a clinic/hospital\n",
    "\n",
    "Hospital appointment where the patient does not turn-up is defined as an appointment in which the patient did not visit for the treatment or cancelled the same day. As these appointments are problematic for practices at all levels of the health care system, patients not turning up is a missed revenue opportunity which cannot be re-captured for the practice, and which contribute to both decreased patient and staff satisfaction. Such appointments negatively impact both patients and care teams. The objective is to predict the probability of a patient not turning up for a medical appointment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnYKuaZZd5pc"
   },
   "source": [
    "## Data:\n",
    "\n",
    "This analysis consists of exploring a dataset containing approximately 100k medical appointments from the Brazilian public health system known as SUS (Single Health System). \n",
    "\n",
    "### Attributes Description:\n",
    "\n",
    "* PatientId: Identification of a patient\n",
    "* AppointmentID: Identification of each appointment\n",
    "* Gender: Male or Female\n",
    "* ScheduledDay: The day someone called or registered the appointment\n",
    "* AppointmentDay: The day of the actual appointment, when they have to visit the doctor\n",
    "* Age: How old is the patient\n",
    "* Neighbourhood: Where the appointment takes place\n",
    "* Scholarship: True or False, indicates if the patient is in the Bolsa Familia program (a social welfare program of the Government of Brazil)\n",
    "* Hypertension: True or False\n",
    "* Diabetes: True or False\n",
    "* Alcoholism: True or False\n",
    "* Handicap: True or False\n",
    "* SMS_received: 1 or more messages sent to the patient\n",
    "* No-show: \"No\" indicates if the patient attended to their appointment and \"Yes\" if they didn't attend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We saw the problrm statement right.. What do you think which type of events or errors cause problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMp6GOCgd5pd"
   },
   "source": [
    "## Error Metric : Recall\n",
    "\n",
    "For this problem, minimizing Type II errors is a priority. A Type II error here is a situation where the algorithm incorrectly posits a patient will turn up for an appointment, but they do not present themselves for the scheduled appointment. The inverse situation, in which the prediction is a patient no-attendance, but they do attend the appointment is less of a burden on the practice and has fewer negative impacts for the patient than not receiving care. The prediction as no-attendance, but the patient does attend, is the Type I error. The most successful algorithm is defined as one with high recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9D_9-5vd5pe"
   },
   "source": [
    "## Importing the required libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "2rqQueyld5pf",
    "outputId": "c0227fbe-5210-4525-a415-9ea7c238c592"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaBoostClassifier\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn_pandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrameMapper\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedShuffleSplit\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwaPK6hed5pm"
   },
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into a dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "colab_type": "code",
    "id": "JkxQjdAwd5po",
    "outputId": "6bb7d40e-4b08-45d3-8566-43e17d9935b4"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the first 5 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ch6U60apd5p2"
   },
   "source": [
    "### Renaming all columns with correct spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRIq6HIzd5p3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RizbADhd5ps"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8KPjjLppd5pt",
    "outputId": "d4e181d0-bd57-4431-83e3-9bc4db68de1a"
   },
   "outputs": [],
   "source": [
    "#shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "oVgxp8j_d5py",
    "outputId": "6803e9b1-edc0-4758-833e-92f0bdbcd927"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Datatypes & Typecasting obvious ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "RTzhGG1dsdst",
    "outputId": "d45169c6-cfce-44f6-dedc-7268d11c3e25"
   },
   "source": [
    "Which features or columns are categorical data type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuvRlTapd5qN"
   },
   "outputs": [],
   "source": [
    "col_names=['PatientId','AppointmentID', 'Gender','Neighbourhood','Scholarship', 'Hypertension','Diabetes','Alcoholism','Handicapped', 'SMS_received', 'No-show']\n",
    "df[col_names] = df[col_names].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check now data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on Patient ID and Appointment ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "wglTkP-8sGU-",
    "outputId": "3c0bbc38-963b-437e-fba4-c12f18285d27"
   },
   "outputs": [],
   "source": [
    "#Number of unique patientID's\n",
    "#print(\"The number of unique Patient ID's in the  data set\")\n",
    "\n",
    "\n",
    "#Number of unique AppointmentID's\n",
    "#print(\"The number of unique Appointment ID's in the  data set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations:__ \n",
    "- Is apppointment id unique? what should we do?\n",
    "\n",
    "- Is Patient ID unique what should we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmHAkt6Od5p7"
   },
   "source": [
    "#### Dropping Zero Variance features like Appointment ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUJ_qq8pd5qX"
   },
   "source": [
    "### Analysing Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Age Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Age counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LphRu2bwd5qc"
   },
   "source": [
    "__Observation__: Age cannot have negative values, So what should we do? Some intelligent guess!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4QP9yVCd5ql"
   },
   "outputs": [],
   "source": [
    "#Let us replace them with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "YjCM-U1md5qo",
    "outputId": "5af707fb-b14a-4d10-a3b8-89ff54c730fd"
   },
   "outputs": [],
   "source": [
    "# Get stats of Age column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Age on No-show attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "6MJ1ndDrsyMX",
    "outputId": "c88c9d05-24f2-41f4-e1ad-0eb2ebd8e564",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['Age'].hist(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ What do you observe from the data?\n",
    "Is it left skewed or right skwed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find for each age category what fraction people cancelled the appointment?\n",
    "\n",
    "1. How can we do that?\n",
    "\n",
    "2. what is the appropriate function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Age  No   Yes     TotalCount  probNoShowUp\n",
    " \n",
    "0     0   2901   639         3540       0.180508\n",
    "\n",
    "1     1   1858   415         2273       0.182578\n",
    "\n",
    "2     2   1366   252         1618       0.155748\n",
    "\n",
    "3     3   1236   277         1513       0.183080\n",
    "\n",
    "4     4   1017   282         1299       0.217090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbOfNoShow(col, target, df, forGraph=True):\n",
    "    crosstab = pd.crosstab(index = df[col], columns = df[target])\n",
    "    crosstab.columns = pd.Index(list(crosstab.columns))\n",
    "    crosstab = crosstab.reset_index() \n",
    "    crosstab['TotalCount'] = crosstab['Yes'] + crosstab['No']\n",
    "    crosstab['probNoShowUp'] = crosstab['Yes'] / crosstab['TotalCount']\n",
    "    \n",
    "    print(crosstab.head())\n",
    "    if forGraph:\n",
    "        return crosstab[[col, 'probNoShowUp']]\n",
    "    else:\n",
    "        return crosstab[[col, 'TotalCount', 'probNoShowUp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = getProbOfNoShow('Age', 'No-show', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to know what fraction of no-shows with age. What should we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lmplot(data = data, x = 'Age', y = 'probNoShowUp', fit_reg = True)\n",
    "#plt.xticks(np.arange(np.min(df['Age']), np.max(df['Age']), 10))\n",
    "#plt.title('Probability of Not Showing up with respect to Age')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bining the age column based on above plot\n",
    "\n",
    "bins = list(np.arange(np.min(df['Age']), np.max(df['Age'])+20, 20))\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = bins[:-1]\n",
    "print(labels)\n",
    "df['Age_binned'] = pd.cut(df['Age'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Age > 100, ['Age','Age_binned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Age'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JKliQt4ukB0"
   },
   "source": [
    "### Analysing Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "8qSHUtuc0qDH",
    "outputId": "4197b257-be3d-44ac-e7e5-5537f5df20dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDjMwLu_wbnI"
   },
   "outputs": [],
   "source": [
    "## Custom Function for Bar Plots\n",
    "\n",
    "def barplot(column,df):\n",
    "    bar_plot1 = sns.countplot(x=column, data=df)\n",
    "    \n",
    "    total = len(df[column])\n",
    "    for p in bar_plot1.patches:\n",
    "        percentage = '{:.2f}%'.format(100 * p.get_height()/total)\n",
    "        height = p.get_height()\n",
    "        bar_plot1.text(p.get_x()+ p.get_width()/2, height + 400, percentage, ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is UNIVARIATE ANALYSIS? Why do we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis on Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "aZA979F71FNl",
    "outputId": "cf75ac9c-456e-4ca8-dd35-db66b5adbbb5"
   },
   "outputs": [],
   "source": [
    "barplot(\"Age_binned\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Scholarship\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Hypertension\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Diabetes\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Handicapped\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Alcoholism\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Gender\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"SMS_received\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: Using Univariate plots on categorical columns, it is very difficult to find out which attribute is related to the target or not. \n",
    "\n",
    "From the above plots, few things can be inferred though\n",
    "- Handicapped attribute can be merged to binary as other levels have less representation and also no proper information shared by the client on other levels.\n",
    "\n",
    "- Chi-square test of independence and Bivariate plots can be meaningful "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning of Handicapped attrribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Handicapped']=df['Handicapped'].astype('int')\n",
    "df.loc[df.Handicapped > 1, 'Handicapped'] = 1\n",
    "df['Handicapped']=df['Handicapped'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Handicapped'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can anyone of you describe what is the purpose of Chi-Square test ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square test of independence\n",
    "\n",
    "The Pearson’s Chi-Square statistical hypothesis is a test for independence between categorical variables.\n",
    "\n",
    "We will perform the test using a mathematical approach and then using Python’s SciPy module.\n",
    "\n",
    "\n",
    "- We start by defining the null hypothesis (H0) which states that there is no relation between the variables. \n",
    "- An alternate hypothesis(H1) would state that there is a significant relation between the two.\n",
    "\n",
    "- Using p-value:\n",
    "    * We define a significance factor to determine whether the relation between the variables is of considerable significance. \n",
    "    * Generally a significance factor or alpha value of 0.05 is chosen. This alpha value denotes the probability of erroneously rejecting H0 when it is true.\n",
    "\n",
    "    * If the p-value for the test comes out to be strictly greater than the alpha value, then H0 holds true.\n",
    "    \n",
    "The chi2_contingency() function of scipy.stats module takes, \n",
    "- Input as the contingency table in 2d array format. \n",
    "- Returns a tuple containing test statistics, the p-value, degrees of freedom and expected table(the one we created from the calculated values) in that order. \n",
    "\n",
    "Hence, we need to compare the obtained p-value with alpha value of 0.05 (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "\n",
    "## conda install -c anaconda statsmodels\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes('category').columns\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add formula for chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chi2_independence(cat_col, target, df, alpha=0.05):\n",
    "    data = df[[target, cat_col]]\n",
    "    tab = sm.stats.Table.from_data(data)\n",
    "    tab = tab.table_orig.to_numpy()\n",
    "    print(f\"---------------{target} Vs {cat_col} Chi Square Test of Independence -------------------\")\n",
    "    print(f\"\\n Contingency table :\\n\")\n",
    "    print(tab)\n",
    "    \n",
    "    stat, p, dof, expected = chi2_contingency(tab)\n",
    "    print(f\"\\n Expected table :\\n\")\n",
    "    print(expected)\n",
    "    \n",
    "    print(f\"The p value returned = {p} and degrees of freedom returned = {dof}\")\n",
    "    \n",
    "    # interpret p-value\n",
    "    print('significance(alpha) = %.3f' % (alpha))\n",
    "\n",
    "    if p <= alpha:\n",
    "        print('Dependent (reject H0)')\n",
    "    else:\n",
    "        print('Independent (fail to reject H0)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    find_chi2_independence(col, 'No-show', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: \n",
    "- PatientId, Neighbourhood, Scholarship, Hypertension, Diabetes, SMS_received, Age_binned attributes tend to influence the target\n",
    "\n",
    "- Gender & Alcoholism doesn't have any dependency with target\n",
    "\n",
    "Let us also run probabilistic approach for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Analysis of Categorical columns on No show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probStatusCategorical(col_list):\n",
    "    rows = []\n",
    "    for item in col_list:\n",
    "        for level in df[item].unique():\n",
    "            row = {'Condition': item}\n",
    "            ## Finding count of rows with that level in categorical column\n",
    "            total = len(df[df[item] == level])\n",
    "            ## Finding count of rows where level and Yes in No-show match\n",
    "            n = len(df[(df[item] == level) & (df['No-show'] == 'Yes')])\n",
    "            row.update({'Level': level, 'Probability': n / total})\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(data = probStatusCategorical(['Diabetes', 'Alcoholism', 'Handicapped', \n",
    "                                          'SMS_received', 'Gender', 'Hypertension','Scholarship']),\n",
    "            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\n",
    "plt.title('Probability of not showing up')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ \n",
    "- 1) Clearly SMS received status and Scholarship Status drive no-show up's\n",
    "- 2) Gender and Alcoholism are not contributing to the target (No-show class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing High Cardinal Categorical Attributes\n",
    "\n",
    "- We have two attributes namely PatientID and Neighbourhood that have high cardinality. Also Chi-square test of independence shows that these two columns influence target variable.\n",
    "\n",
    "- But given some limitations with known techniques under ML till date, we would like to handle these variables using Categorical Embeddings which will be taught in Deep Learning Module. Hence proceeding further we will not be using these variables in our analysis\n",
    "\n",
    "- Before we step in to DL module, we can still use pandas techniques like crosstab to find the probabilistic nature of these variables on No-show attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = getProbOfNoShow('PatientId', 'No-show', df, forGraph=False)\n",
    "patient_df.to_csv('patientID_VS_NoShow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_df = getProbOfNoShow('Neighbourhood', 'No-show', df, forGraph=False)\n",
    "neigh_df.to_csv('Neighbourhood_VS_NoShow.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping off the above columns\n",
    "df = df.drop(['Neighbourhood','PatientId','Gender','Alcoholism'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFrmfKPad5qt"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MPYEfJWd5qu"
   },
   "source": [
    "### Generating new features from scheduled and appointment date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRuXxZ8Qd5qv"
   },
   "outputs": [],
   "source": [
    "df['Scheduled_DateTime'] = pd.to_datetime(df['ScheduledDay'])\n",
    "df['Scheduled_date'] = df['Scheduled_DateTime'].dt.date\n",
    "df['Scheduled_month']=df['Scheduled_DateTime'].dt.month\n",
    "df['Scheduled_hour'] = df['Scheduled_DateTime'].dt.hour\n",
    "df['Scheduled_weekday']=df['Scheduled_DateTime'].dt.weekday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmJvY-qEd5qz"
   },
   "outputs": [],
   "source": [
    "df['Appointment_DateTime'] = pd.to_datetime(df['AppointmentDay'])\n",
    "df['Appointment_date'] = df['Appointment_DateTime'].dt.date\n",
    "df['Appointment_month']=df['Appointment_DateTime'].dt.month\n",
    "df['Appointment_hour'] = df['Appointment_DateTime'].dt.hour\n",
    "df['Appointment_weekday']=df['Appointment_DateTime'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF7mKBNRljfb"
   },
   "outputs": [],
   "source": [
    "np.min(df['Scheduled_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rH7t6ogFlji0"
   },
   "outputs": [],
   "source": [
    "np.max(df['Scheduled_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Appointment_DateTime', 'Appointment_date', 'Appointment_month', 'Appointment_hour', 'Appointment_weekday']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55j60oyfd5rR"
   },
   "source": [
    "### Creating 'number of days to appointment' \n",
    "\n",
    "This feature may have an impact on our target feature as we tend to forget,hence retrieving new feature from scheduled date and appointment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQ9IENKrd5rS"
   },
   "outputs": [],
   "source": [
    "fn = lambda row: (row.Appointment_date - row.Scheduled_date).days\n",
    "df['num_of_days'] = df.apply (fn, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can anyone explain what above two line of code does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "Fa209PH1d5ra",
    "outputId": "1df7d490-cad1-401e-ea24-3f7622eaeb97"
   },
   "outputs": [],
   "source": [
    "df.num_of_days.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTRzMFOHd5re"
   },
   "source": [
    "__Observation__: We can see some negative values indicating there might be some outliers in the data as appointment date cannot happen before the scheduled date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.num_of_days < 0, ['num_of_days']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CipDrGPud5q6"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['ScheduledDay','Scheduled_DateTime','Scheduled_date'], axis=1)\n",
    "df = df.drop(['AppointmentDay','Appointment_DateTime','Appointment_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "F1IC4UlDd5rM",
    "outputId": "07603e9c-7678-4312-c824-b01f1fefdb61"
   },
   "outputs": [],
   "source": [
    "#0:MONDAY 1:TUESDAY 2:WEDNESDAY 3:THURSDAY 4:FRIDAY 5:SATURDAY\n",
    "barplot(\"Appointment_weekday\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Appointment_hour\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Scheduled_hour\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Appointment_month\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(\"Scheduled_month\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ appointment hour has zero variance. So can be _________????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Appointment_hour'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typecasting newly created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Scheduled_month', 'Scheduled_weekday', 'Appointment_month', 'Appointment_weekday']:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of num_days on No-show attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Think!!! Why number of days between schedule date and appointment date matters in No-Shows??__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = getProbOfNoShow('num_of_days', 'No-show', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What do you obseve?__\n",
    "\n",
    "__Is there any relation between no-shows and number of days between scheduled date and appointment date?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = data, x = 'num_of_days', y = 'probNoShowUp', fit_reg = True)\n",
    "# plt.xticks(np.arange(np.min(df['num_days']), np.max(df['num_days']), 10))\n",
    "plt.title('Probability of Not Showing up with respect to Num of Days to appointment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ Num of days prior to appointment has no clear pattern in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Scheduled_hour on No-show attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = getProbOfNoShow('Scheduled_hour', 'No-show', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data = data, x = 'Scheduled_hour', y = 'probNoShowUp', fit_reg = True)\n",
    "# plt.xticks(np.arange(np.min(df['num_days']), np.max(df['num_days']), 10))\n",
    "plt.title('Probability of Not Showing up wrt Hour the appointment is booked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: There is clear trend in above graph, showcasing the effect of scheduled hour on no-show up. We can bin Scheduled hour with bin size of 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5ty54wzd5r-"
   },
   "outputs": [],
   "source": [
    "#Binning of Scheduled_time\n",
    "def get_session_of_day(x):\n",
    "    if x < 8:\n",
    "        return 'EarlyMorning'\n",
    "    elif (x >= 8) and (x < 12 ):\n",
    "        return 'Morning'\n",
    "    elif (x >= 12) and (x < 16):\n",
    "        return'Noon'\n",
    "    elif (x >= 16) and (x < 20) :\n",
    "        return 'Eve'\n",
    "    elif (x >= 20) and (x < 24):\n",
    "        return'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJc4z6ACd5sB"
   },
   "outputs": [],
   "source": [
    "df['Scheduled_session_of_day'] = df['Scheduled_hour'].apply(get_session_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYrDYp4hd5sF"
   },
   "outputs": [],
   "source": [
    "df=df.drop('Scheduled_hour',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "tQPpHLIvd5sI",
    "outputId": "faf8234d-3a33-4469-d175-8a707fd99705"
   },
   "outputs": [],
   "source": [
    "df['Scheduled_session_of_day']=df['Scheduled_session_of_day'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Chisquare test of independence on newly added categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Scheduled_month', 'Scheduled_weekday', 'Appointment_month', 'Appointment_weekday', 'Scheduled_session_of_day']:\n",
    "    find_chi2_independence(col, 'No-show', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__ We see most of the newly engineered features show some relation w.r.t to target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsOAKvSaCmzk"
   },
   "source": [
    "### Probabilistic Analysis of newly created features\n",
    "\n",
    "##### Scheduled Month and Appointment Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsOAKvSaCmzk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(data = probStatusCategorical(['Scheduled_month', 'Appointment_month']),\n",
    "            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\n",
    "plt.title('Probability of not showing up')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation__: Suprised to see that appointments are allowed/booked for only three months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scheduled Weekday and Appointment Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eh7JH-i9YWuB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(data = probStatusCategorical(['Scheduled_weekday', 'Appointment_weekday']),\n",
    "            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\n",
    "plt.title('Probability of not showing up')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAJ3DxjVWFGq"
   },
   "source": [
    "__Observation__: Weekday seems to have similar/equal pattern on no show ups. \n",
    "\n",
    "Some interesting insights are\n",
    "- Significantly less people called/scheduled for an appointment on Saturday. \n",
    "- More people tend to not show up on Saturday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwqlMd6Xd5sc"
   },
   "source": [
    "## Checking  target feature distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "zzMKi_Upd5sc",
    "outputId": "a5f55472-5e09-4994-bf20-d2306bd38dc8"
   },
   "outputs": [],
   "source": [
    "barplot(\"No-show\",df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "EfLDFq-Ud5sg",
    "outputId": "15f53008-c720-46e1-c96a-ab13271f77b6"
   },
   "outputs": [],
   "source": [
    "print(\"The count distribution target classes is as below:\")\n",
    "\n",
    "df['No-show'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "iriVBDOdd5sm",
    "outputId": "6e019257-8365-4dbc-bd95-9e71442f8356"
   },
   "outputs": [],
   "source": [
    "print(\"The percentage distribution target classes is as below:\")\n",
    "\n",
    "df['No-show'].value_counts('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qfZqh2TVKMlR"
   },
   "source": [
    "### Replacing Yes/No in the target column with 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "yVeCsPgqJwCI",
    "outputId": "e53993ba-dfab-4732-fedc-ef8e3d3c83f1"
   },
   "outputs": [],
   "source": [
    "df['No-show'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "41tjLHDUJm5n",
    "outputId": "753d84f0-ad77-4f1e-9d7c-267c3c61138c"
   },
   "outputs": [],
   "source": [
    "df['No-show'] = df['No-show'].replace(to_replace=['No', 'Yes'], value=[0, 1])\n",
    "df['No-show'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0o4R4QSYxvB"
   },
   "source": [
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nu40qDCMYhJz"
   },
   "outputs": [],
   "source": [
    "y=df[\"No-show\"]\n",
    "X=df.drop('No-show', axis=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20,random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "rNkF067nZC_K",
    "outputId": "f83f2d44-0580-4190-eb7e-31384307435e"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhXLf6cyZJ9t"
   },
   "source": [
    "## Split the attributes into numerical and categorical types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lyFFCBJ-ZMtL",
    "outputId": "faaff1f8-3369-4cfb-839e-7154666213f6"
   },
   "outputs": [],
   "source": [
    "num_attr=X_train.select_dtypes(['int64']).columns\n",
    "num_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "v77MaP0MZMw1",
    "outputId": "df106421-0e5a-4b31-f4ad-5f6562113816"
   },
   "outputs": [],
   "source": [
    "cat_attr = X_train.select_dtypes('category').columns\n",
    "cat_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78FZr8TCEtJP"
   },
   "source": [
    "# Preparation for Model buillding\n",
    "\n",
    "## Imputing missing values with median for numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MElasrMwaRX-"
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "imputer = imputer.fit(X_train[num_attr])\n",
    "\n",
    "X_train[num_attr] = imputer.transform(X_train[num_attr])\n",
    "X_val[num_attr] = imputer.transform(X_val[num_attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "FQVc5HtDFSsz",
    "outputId": "261ca58f-e6b6-4be3-b4db-c68ddde53a25"
   },
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())\n",
    "print(X_val.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing values with mode for categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8qkcZOodt4S"
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imputer = imputer.fit(X_train[cat_attr])\n",
    "\n",
    "X_train[cat_attr] = imputer.transform(X_train[cat_attr])\n",
    "X_val[cat_attr] = imputer.transform(X_val[cat_attr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HC6ewXEwFoqn"
   },
   "source": [
    "### Standardizing the numerical attributes and One-hot encoding categorical attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameMapper, a class for mapping pandas data frame columns to different sklearn transformations\n",
    "mapper = DataFrameMapper(\n",
    "  [([continuous_col], StandardScaler()) for continuous_col in num_attr] +\n",
    "  [([categorical_col], OneHotEncoder(handle_unknown='error')) for categorical_col in cat_attr]\n",
    ", df_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mapper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.fit(X_train)\n",
    "\n",
    "X_train_final = mapper.transform(X_train)\n",
    "X_val_final = mapper.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaCrCouj867P"
   },
   "source": [
    "### Defining Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CR_CM(train_actual,train_predicted,test_actual,test_predicted):\n",
    "    print('''\n",
    "         ========================================\n",
    "           CLASSIFICATION REPORT FOR TRAIN DATA\n",
    "         ========================================\n",
    "        ''')\n",
    "    print(classification_report(train_actual, train_predicted, digits=4))\n",
    "\n",
    "    print('''\n",
    "             =============================================\n",
    "               CLASSIFICATION REPORT FOR VALIDATION DATA\n",
    "             =============================================\n",
    "            ''')\n",
    "    print(classification_report(test_actual, test_predicted, digits=4))\n",
    "\n",
    "    print('''\n",
    "             ========================================\n",
    "               Confusion Matrix FOR TRAIN DATA\n",
    "             ========================================\n",
    "            ''')\n",
    "    print(confusion_matrix(train_actual, train_predicted))\n",
    "\n",
    "    print('''\n",
    "             =============================================\n",
    "               Confusion matrix FOR VALIDATION DATA\n",
    "             =============================================\n",
    "            ''')\n",
    "    print(confusion_matrix(test_actual, test_predicted))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T10:32:34.925827Z",
     "start_time": "2020-03-06T10:32:34.910844Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JYJp3Qo5viNe"
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['Model','Train_Accuracy','Train_Recall','Train_Precision','Train_F1_Score','Test_Accuracy','Test_Recall','Test_Precision','Test_F1_Score'])\n",
    "\n",
    "def get_metrics(train_actual,train_predicted,test_actual,test_predicted,model_description,dataframe):\n",
    "    get_CR_CM(train_actual,train_predicted,test_actual,test_predicted)\n",
    "    train_accuracy = accuracy_score(train_actual,train_predicted)\n",
    "    train_recall   = recall_score(train_actual,train_predicted)\n",
    "    train_precision= precision_score(train_actual,train_predicted)\n",
    "    train_f1score  = f1_score(train_actual,train_predicted)\n",
    "    test_accuracy = accuracy_score(test_actual,test_predicted)\n",
    "    test_recall   = recall_score(test_actual,test_predicted)\n",
    "    test_precision= precision_score(test_actual,test_predicted)\n",
    "    test_f1score  = f1_score(test_actual,test_predicted)\n",
    "    dataframe = dataframe.append(pd.Series([model_description, train_accuracy,train_recall,train_precision,train_f1score,\n",
    "                                            test_accuracy,test_recall,test_precision,test_f1score],\n",
    "                                           index=scores.columns ), ignore_index=True)\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_QDNJJObgfI"
   },
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-hNOw6tMd5vZ"
   },
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kU2kFgufd5vZ"
   },
   "outputs": [],
   "source": [
    "log_mod = LogisticRegression(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "_7G00N2kd5ve",
    "outputId": "7c43edc8-19e0-45b2-8f08-e0e56d1aff2a"
   },
   "outputs": [],
   "source": [
    "log_mod.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "677azofTd5vi"
   },
   "outputs": [],
   "source": [
    "y_pred_train = log_mod.predict(X_train_final)\n",
    "y_pred_val = log_mod.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbZMZ6ied5vl"
   },
   "source": [
    "##### Evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"LogisticRegression\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Handling Imbalanced Data_\n",
    "\n",
    "### Class Weights of loss function\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression with class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kU2kFgufd5vZ"
   },
   "outputs": [],
   "source": [
    "log_mod = LogisticRegression(class_weight='balanced', random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "_7G00N2kd5ve",
    "outputId": "7c43edc8-19e0-45b2-8f08-e0e56d1aff2a"
   },
   "outputs": [],
   "source": [
    "log_mod.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "677azofTd5vi"
   },
   "outputs": [],
   "source": [
    "y_pred_train = log_mod.predict(X_train_final)\n",
    "y_pred_val = log_mod.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbZMZ6ied5vl"
   },
   "source": [
    "##### Evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "u9hLRmlZd5vm",
    "outputId": "5e84337d-d333-4619-baf7-44d2d5d6e55f"
   },
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"LogisticRegression_Balanced\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WW1d5_zzd5wT"
   },
   "source": [
    "### 3. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "42_Uze_ud5wV",
    "outputId": "6feaa72f-e3d5-46a8-ee3d-d7cfcedc85b9"
   },
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf_dt = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf_dt.fit(X_train_final,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnZ-6AoDd5wZ"
   },
   "outputs": [],
   "source": [
    "y_pred_train = clf_dt.predict(X_train_final)\n",
    "y_pred_val = clf_dt.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FC9zvT6d5wc"
   },
   "source": [
    "##### Evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "eKaY_FKbd5wd",
    "outputId": "14b84934-ccdd-4a5a-985a-0744b8407822"
   },
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"DecisionTree\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U4nOsqeQd5xt"
   },
   "source": [
    "### 4. Decision Tree with Grid-Search CV - Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Db_siKVd5xu"
   },
   "outputs": [],
   "source": [
    "# set of parameters to test\n",
    "param_grid = {\"class_weight\":['balanced', None],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              \"max_depth\": [3, 5, 6],\n",
    "              \"min_samples_leaf\": [2, 5, 10],\n",
    "               \"max_leaf_nodes\": [None, 5, 10, 20]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(random_state=123)\n",
    "clf_dt_grid = GridSearchCV(dt, param_grid,cv=5,scoring='recall')\n",
    "clf_dt_grid.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "aeaYPcF-d5x3",
    "outputId": "b767a2c8-827a-4948-d992-30f7bb314788"
   },
   "outputs": [],
   "source": [
    "clf_dt_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BFwnRpxd5x6"
   },
   "outputs": [],
   "source": [
    "y_pred_train = clf_dt_grid.predict(X_train_final)\n",
    "y_pred_val = clf_dt_grid.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "ZrPvmltvd5x-",
    "outputId": "90662edd-7ba8-4fa7-c8c5-6bf575be210a"
   },
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"DecisionTree_BestParameters\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision Tree with RandomizedSearchCV - Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "\n",
    "clf3_dt = DecisionTreeClassifier(random_state=123, class_weight='balanced') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaf_nodes = np.random.normal(loc=5, scale=1, size=5).astype(int)\n",
    "max_leaf_nodes[max_leaf_nodes <1] = 1\n",
    "print(max_leaf_nodes)\n",
    "max_depth = np.random.uniform(2,5,4).astype(int)\n",
    "print(max_depth)\n",
    "min_samples_split = np.random.uniform(2, 6, 5).astype(int)\n",
    "print(min_samples_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Up Hyperparameter Distributions\n",
    "\n",
    "# normally distributed max_leaf_nodes, with mean 5 stddev 1\n",
    "max_leaf_nodes = np.random.normal(loc=5, scale=1, size=5).astype(int)\n",
    "\n",
    "# uniform distribution from 2 to 5 \n",
    "max_depth = np.random.uniform(2,5,4).astype(int)\n",
    "\n",
    "# uniform distribution from 2 to 6\n",
    "min_samples_split = np.random.uniform(2, 6, 5).astype(int)\n",
    "\n",
    "model_params = {\n",
    "    'max_depth': list(max_depth),\n",
    "    'max_leaf_nodes': list(max_leaf_nodes),\n",
    "    'min_samples_split': list(min_samples_split)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_random = RandomizedSearchCV(estimator=clf3_dt, param_distributions=model_params, n_iter=600, cv=5, scoring='recall', n_jobs=-1)\n",
    "clf_random.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_random.best_score_, clf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_random = clf_random.best_estimator_\n",
    "print(clf_dt_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf_dt_random.predict(X_train_final)\n",
    "y_pred_val = clf_dt_random.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"DecisionTree_RandomizedSearch\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJPF5VqGd5yQ"
   },
   "source": [
    "### 6. Adaboost Classifier\n",
    "\n",
    "The most important parameters are base_estimator, n_estimators, and learning_rate.\n",
    "-  **base_estimator** is the learning algorithm to use to train the weak models. This will almost always not needed to be changed because by far the most common learner to use with AdaBoost is a decision tree – this parameter’s default argument.\n",
    "-  **n_estimators** is the number of models to iteratively train.\n",
    "-  **learning_rate** is the contribution of each model to the weights and defaults to 1. Reducing the learning rate will mean the weights will be increased or decreased to a small degree, forcing the model train slower (but sometimes resulting in better performance scores).\n",
    "-  **loss** is exclusive to AdaBoostRegressor and sets the loss function to use when updating weights. This defaults to a linear loss function however can be changed to square or exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "VmBV0srQd5yw",
    "outputId": "4506efb8-d61b-48f0-a29f-546eee64b170"
   },
   "outputs": [],
   "source": [
    "clf_ada = AdaBoostClassifier(DecisionTreeClassifier(criterion=\"gini\",class_weight='balanced'),n_estimators=100,learning_rate = 0.4,random_state=123)\n",
    "clf_ada.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqDZPxeqd5y1"
   },
   "outputs": [],
   "source": [
    "y_pred_train = clf_ada.predict(X_train_final)\n",
    "y_pred_val = clf_ada.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"Adaboost\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Adaboost Classifier with Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "param_grid = {'n_estimators' : [100, 200, 300],\n",
    "              'learning_rate' : [0.2, 0.3, 0.4]}\n",
    "\n",
    "clf_grid_ada = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier()), param_grid, scoring='recall',n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_grid_ada.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ada_model = clf_grid_ada.best_estimator_\n",
    "print(best_ada_model)\n",
    "print (clf_grid_ada.best_score_, clf_grid_ada.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_ada_model.predict(X_train_final)\n",
    "y_pred_val = best_ada_model.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"Adaboost_BestParameters\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important parameters are learning_rate, n_estimators and subsample\n",
    "- **learning_rate**\n",
    "    -  This determines the impact of each tree on the final outcome (step 2.4). GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n",
    "    -  Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n",
    "    -  Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n",
    "- **n_estimators**\n",
    "    -  The number of sequential trees to be modeled (step 2)\n",
    "    -  Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n",
    "- **subsample**\n",
    "    -  The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "    -  Values slightly less than 1 make the model robust by reducing the variance.\n",
    "    -  Typical values ~0.8 generally work fine but can be fine-tuned further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GBM = GradientBoostingClassifier(n_estimators=50,\n",
    "                                       learning_rate=0.3,\n",
    "                                       subsample=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_GBM.fit(X=X_train_final, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf_GBM.predict(X_train_final)\n",
    "y_pred_val = clf_GBM.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"GBM\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance using Synthetic Minority Oversampling Technique\n",
    "#### Oversample Using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r'/home/datasets/lab/Batch85/MachineLearning/Project/img/SMOTE.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GS2gnzI1d5v1"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=123)\n",
    "os_train_X, os_train_y = smote.fit_sample(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "UMskhdRcNBtf",
    "outputId": "28b1d7d0-13b2-46a7-f5c8-114daca1f809"
   },
   "outputs": [],
   "source": [
    "# observe that data has been balanced\n",
    "pd.Series(os_train_y).value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_train_X = pd.DataFrame(data=os_train_X)\n",
    "os_train_y= pd.DataFrame(data=os_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os_train_y.shape)\n",
    "print(os_train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_train_y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_train_X))\n",
    "print(\"Number of 'No-show' patients in oversampled data\",len(os_train_y[os_train_y['No-show']==1]))\n",
    "print(\"Number of non 'No-show' patients \",len(os_train_y[os_train_y['No-show']==0]))\n",
    "\n",
    "print(\"Proportion of 'No-show' patients in oversampled data is\",len(os_train_y[os_train_y['No-show']==1])/len(os_train_X))\n",
    "print(\"Proportion of non 'No-show' patients in oversampled data is\",len(os_train_y[os_train_y['No-show']==0])/len(os_train_X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. GBM with SMOTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inX3Rx-Kd5v4"
   },
   "outputs": [],
   "source": [
    "clf_GBM_smot = GradientBoostingClassifier(n_estimators=100,\n",
    "                                       learning_rate=0.01,\n",
    "                                       subsample=0.8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "fM8luwkHd5v6",
    "outputId": "552973f9-89d7-4c5c-f910-ceb644300454"
   },
   "outputs": [],
   "source": [
    "clf_GBM_smot.fit(os_train_X, os_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pg7r-yoPd5v-"
   },
   "outputs": [],
   "source": [
    "y_pred_train = clf_GBM_smot.predict(os_train_X)\n",
    "y_pred_val = clf_GBM_smot.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pj2wz7c1d5wC"
   },
   "source": [
    "##### Evaluating the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "FIIRNJuPd5wD",
    "outputId": "2501fb52-7391-4507-9ac2-e6d5dd1a9c64"
   },
   "outputs": [],
   "source": [
    "scores = get_metrics(os_train_y,y_pred_train,y_val,y_pred_val,\"GBM_SMOTe\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. XGBoost Classifier\n",
    "XGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:\n",
    "#### Create XGBoost Classifier\n",
    "\n",
    "There are different hyperparameters that we can tune and the parametres are different from baselearner to baselearner. \n",
    "<br>In tree based learners, which are the most common ones in xgboost applications, the following are the most commonly tuned hyperparameters:\n",
    "\n",
    "-  **learning rate/eta:** governs how quickly the model fits the residual error using additional base learners. If it is a smaller learning rate, it will need more boosting rounds, hence more time, to achieve the same reduction in residual error as one with larger learning rate. Typically, it lies between 0.01 – 0.3\n",
    "-  **max_depth:** max depth per tree. This controls how deep our tree can grow. The Larger the depth, more complex the model will be and higher chances of overfitting. Larger data sets require deep trees to learn the rules from data. Default = 6.\n",
    "-  **subsample:** % samples used per tree. This is the fraction of the total training set that can be used in any boosting round. Low value may lead to underfitting issues. A very high value can cause over-fitting problems.\n",
    "-  **colsample_bytree:** % features used per tree. This is the fraction of the number of columns that we can use in any boosting round. A smaller value is an additional regularization and a larger value may be cause overfitting issues.\n",
    "-  **n_estimators:** number of estimators (base learners). This is the number of boosting rounds.\n",
    "- **scale_pos_weight:** [default=1]\n",
    "\n",
    "    - Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "    - A typical value to consider: sum(negative instances) / sum(positive instances)\n",
    "\n",
    "- The XGBoost documentation suggests a fast way to estimate this value using the training dataset as the total number of examples in the majority class divided by the total number of examples in the minority class.\n",
    "\n",
    "    - scale_pos_weight = total_negative_examples / total_positive_examples\n",
    "\n",
    "The three hyperparameters below are regularization hyperparameters.\n",
    "\n",
    "-  **gamma:** min loss reduction to create new tree split. default = 0 means no regularization.\n",
    "-  **lambda:** L2 reg on leaf weights. Equivalent to Ridge regression.\n",
    "-  **alpha:** L1 reg on leaf weights. Equivalent to Lasso regression.\n",
    "\n",
    "\n",
    "Refer: https://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB = XGBClassifier(n_estimators=400, gamma=0.5,learning_rate=0.1,n_jobs=-1)\n",
    "clf_XGB.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = clf_XGB.predict(X_train_final)\n",
    "y_pred_val = clf_XGB.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"XGBoost\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted XGBoost for Class Imbalance -\n",
    "\n",
    "Although the XGBoost algorithm performs well for a wide range of challenging problems, it offers a large number of hyperparameters, many of which require tuning in order to get the most out of the algorithm on a given dataset.\n",
    "\n",
    "The implementation provides a hyperparameter designed to tune the behavior of the algorithm for imbalanced classification problems; this is the __scale_pos_weight__ hyperparameter.\n",
    "\n",
    "By default, the scale_pos_weight hyperparameter is set to the value of 1.0 and has the effect of weighing the balance of positive examples, relative to negative examples when boosting decision trees. For an imbalanced binary classification dataset, the negative class refers to the majority class (class 0) and the positive class refers to the minority class (class 1).\n",
    "\n",
    "XGBoost is trained to minimize a loss function and the “gradient” in gradient boosting refers to the steepness of this loss function, e.g. the amount of error. A small gradient means a small error and, in turn, a small change to the model to correct the error. A large error gradient during training in turn results in a large correction.\n",
    "\n",
    "* Small Gradient: Small error or correction to the model.\n",
    "* Large Gradient: Large error or correction to the model.\n",
    "\n",
    "Gradients are used as the basis for fitting subsequent trees added to boost or correct errors made by the existing state of the ensemble of decision trees.\n",
    "\n",
    "The scale_pos_weight value is used to scale the gradient for the positive class.\n",
    "\n",
    "This has the effect of scaling errors made by the model during training on the positive class and encourages the model to over-correct them. In turn, this can help the model achieve better performance when making predictions on the positive class. Pushed too far, it may result in the model overfitting the positive class at the cost of worse performance on the negative class or both classes.\n",
    "\n",
    "As such, the scale_pos_weight can be used to train a class-weighted or cost-sensitive version of XGBoost for imbalanced classification.\n",
    "\n",
    "##### https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "##### https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count examples in each class\n",
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "# estimate scale_pos_weight value\n",
    "estimate = counter[0] / counter[1]\n",
    "print('Estimate:', estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. XGBoost with Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_XGB_grid = XGBClassifier()\n",
    " \n",
    "# Use a grid over parameters of interest\n",
    "param_grid = {\n",
    "     'colsample_bytree': np.linspace(0.6, 0.8, 2),\n",
    "     'n_estimators':[100, 200],\n",
    "     'max_depth': [3, 4],\n",
    "     'gamma': [0.2,0.3,0.4],\n",
    "     'learning_rate': [0.001, 0.01, 0.1, 1, 10],\n",
    "     'scale_pos_weight':[4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "CV_XGB = GridSearchCV(estimator=clf_XGB_grid, param_grid=param_grid, n_jobs=-1, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time CV_XGB.fit(X = X_train_final, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_xgb_model = CV_XGB.best_estimator_\n",
    "print (CV_XGB.best_score_, CV_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=best_xgb_model.predict(X_train_final)\n",
    "y_pred_val=best_xgb_model.predict(X_val_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"XGBoost_BestParameters\",scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.to_csv('Model_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_ztKGNpd50G"
   },
   "source": [
    "## Decision Time!!\n",
    "\n",
    "##### What is your final take on models? Which model should we choose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "* https://github.com/scikit-learn-contrib/sklearn-pandas\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "* https://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A':[1,2,2,2,2], 'B':[3,3,4,4,4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.crosstab(df['A'],df['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(A))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Index(list(A.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.columns = pd.Index(list(A.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.columns\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(index = df[col], columns = df[target])\n",
    "    crosstab.columns = pd.Index(list(crosstab.columns))\n",
    "    crosstab = crosstab.reset_index() \n",
    "    crosstab['TotalCount'] = crosstab['Yes'] + crosstab['No']\n",
    "    crosstab['probNoShowUp'] = crosstab['Yes'] / crosstab['TotalCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Prediction_of_Patient_No_show_Activity_03_09_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
